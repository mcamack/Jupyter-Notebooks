{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harry Potter Text Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Harry Potter Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.glozman.com/textpages.html\n",
    "    \n",
    "# Harry Potter 1 - Sorcerer's Stone.txt\n",
    "# Harry Potter 2 - Chamber of Secrets.txt\n",
    "# Harry Potter 3 - The Prisoner Of Azkaban.txt\n",
    "# Harry Potter 4 - The Goblet Of Fire.txt\n",
    "# Harry Potter 5 - Order of the Phoenix.txt\n",
    "# Harry Potter 6 - The Half Blood Prince.txt\n",
    "# Harry Potter 7 - Deathly Hollows.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts/HarryPotter1-SorcerersStone.txt\", \"r\") as f:\n",
    "    text = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus length: 442745  total chars: 54'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "'corpus length: {}  total chars: {}'.format(len(text), len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry potter and the sorcerer's stone \n",
      "\n",
      "chapter one \n",
      "\n",
      "the boy who lived \n",
      "\n",
      "mr. and mrs. dursley, of n\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a training and test dataset. Take 40 characters and then save the 41st character. We will teach the model that a certain 40 char sequence should generate the 41st char. Use a step size of 3 so there is overlap in the training set and we get a lot more 40/41 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  147569\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i+maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "    \n",
    "print(\"sequences: \", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry potter and the sorcerer's stone \n",
      "\n",
      "\n",
      "ry potter and the sorcerer's stone \n",
      "\n",
      "cha\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    }
   ],
   "source": [
    "print(next_chars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 40, 256)           318464    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 40, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 108)               27756     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 54)                5886      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 54)                0         \n",
      "=================================================================\n",
      "Total params: 1,402,730\n",
      "Trainable params: 1,402,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, recurrent_dropout=0.0, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "model.add(LSTM(256, recurrent_dropout=0.0, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "model.add(LSTM(256, recurrent_dropout=0.0,  input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(2*len(chars)))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "147569/147569 [==============================] - 35s 240us/step - loss: 2.8068\n",
      "Epoch 2/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 2.1695\n",
      "Epoch 3/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 1.8816\n",
      "Epoch 4/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 1.6884\n",
      "Epoch 5/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 1.5492\n",
      "Epoch 6/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 1.4441\n",
      "Epoch 7/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 1.3570\n",
      "Epoch 8/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 1.2845\n",
      "Epoch 9/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 1.2135\n",
      "Epoch 10/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 1.1454\n",
      "Epoch 11/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 1.0750\n",
      "Epoch 12/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 1.0077\n",
      "Epoch 13/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.9373\n",
      "Epoch 14/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 0.8653\n",
      "Epoch 15/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 0.7913\n",
      "Epoch 16/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.7167\n",
      "Epoch 17/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.6445\n",
      "Epoch 18/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 0.5714\n",
      "Epoch 19/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 0.5034\n",
      "Epoch 20/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.4400\n",
      "Epoch 21/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.3816\n",
      "Epoch 22/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.3327\n",
      "Epoch 23/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 0.2876\n",
      "Epoch 24/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.2528\n",
      "Epoch 25/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.2246\n",
      "Epoch 26/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.2020\n",
      "Epoch 27/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.1827\n",
      "Epoch 28/100\n",
      "147569/147569 [==============================] - 35s 234us/step - loss: 0.1687\n",
      "Epoch 29/100\n",
      "147569/147569 [==============================] - 35s 234us/step - loss: 0.1581\n",
      "Epoch 30/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.1483\n",
      "Epoch 31/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.1406\n",
      "Epoch 32/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.1343\n",
      "Epoch 33/100\n",
      "147569/147569 [==============================] - 35s 234us/step - loss: 0.1289\n",
      "Epoch 34/100\n",
      "147569/147569 [==============================] - 34s 234us/step - loss: 0.1250\n",
      "Epoch 35/100\n",
      "147569/147569 [==============================] - 34s 234us/step - loss: 0.1207\n",
      "Epoch 36/100\n",
      "147569/147569 [==============================] - 35s 234us/step - loss: 0.1168\n",
      "Epoch 37/100\n",
      "147569/147569 [==============================] - 34s 234us/step - loss: 0.1138\n",
      "Epoch 38/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.1094\n",
      "Epoch 39/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.1096\n",
      "Epoch 40/100\n",
      "147569/147569 [==============================] - 35s 238us/step - loss: 0.1056\n",
      "Epoch 41/100\n",
      "147569/147569 [==============================] - 35s 234us/step - loss: 0.1015\n",
      "Epoch 42/100\n",
      "147569/147569 [==============================] - 35s 235us/step - loss: 0.1019\n",
      "Epoch 43/100\n",
      "147569/147569 [==============================] - 35s 235us/step - loss: 0.0987\n",
      "Epoch 44/100\n",
      "147569/147569 [==============================] - 35s 235us/step - loss: 0.0965\n",
      "Epoch 45/100\n",
      "147569/147569 [==============================] - 35s 235us/step - loss: 0.0958\n",
      "Epoch 46/100\n",
      "147569/147569 [==============================] - 34s 234us/step - loss: 0.0924\n",
      "Epoch 47/100\n",
      "147569/147569 [==============================] - 35s 234us/step - loss: 0.0923\n",
      "Epoch 48/100\n",
      "147569/147569 [==============================] - 35s 236us/step - loss: 0.0898\n",
      "Epoch 49/100\n",
      "147569/147569 [==============================] - 34s 234us/step - loss: 0.0893\n",
      "Epoch 50/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0847\n",
      "Epoch 51/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0856\n",
      "Epoch 52/100\n",
      "147569/147569 [==============================] - 35s 235us/step - loss: 0.0847\n",
      "Epoch 53/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0831\n",
      "Epoch 54/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0824\n",
      "Epoch 55/100\n",
      "147569/147569 [==============================] - 35s 235us/step - loss: 0.0809\n",
      "Epoch 56/100\n",
      "147569/147569 [==============================] - 35s 234us/step - loss: 0.0815\n",
      "Epoch 57/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0810\n",
      "Epoch 58/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0781\n",
      "Epoch 59/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0763\n",
      "Epoch 60/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0791\n",
      "Epoch 61/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0770\n",
      "Epoch 62/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0754\n",
      "Epoch 63/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0746\n",
      "Epoch 64/100\n",
      "147569/147569 [==============================] - 34s 234us/step - loss: 0.0745\n",
      "Epoch 65/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0743\n",
      "Epoch 66/100\n",
      "147569/147569 [==============================] - 34s 234us/step - loss: 0.0715\n",
      "Epoch 67/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0737\n",
      "Epoch 68/100\n",
      "147569/147569 [==============================] - 35s 234us/step - loss: 0.0728\n",
      "Epoch 69/100\n",
      "147569/147569 [==============================] - 35s 235us/step - loss: 0.0693\n",
      "Epoch 70/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0697\n",
      "Epoch 71/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0694\n",
      "Epoch 72/100\n",
      "147569/147569 [==============================] - 35s 236us/step - loss: 0.0703\n",
      "Epoch 73/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0682\n",
      "Epoch 74/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0662\n",
      "Epoch 75/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 0.0670\n",
      "Epoch 76/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0684\n",
      "Epoch 77/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0648\n",
      "Epoch 78/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0650\n",
      "Epoch 79/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0658\n",
      "Epoch 80/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0646\n",
      "Epoch 81/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0643\n",
      "Epoch 82/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0651\n",
      "Epoch 83/100\n",
      "147569/147569 [==============================] - 34s 231us/step - loss: 0.0644\n",
      "Epoch 84/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0637\n",
      "Epoch 85/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0626\n",
      "Epoch 86/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0622\n",
      "Epoch 87/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0633\n",
      "Epoch 88/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0607\n",
      "Epoch 89/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0609\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0606\n",
      "Epoch 91/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0573\n",
      "Epoch 92/100\n",
      "147569/147569 [==============================] - 35s 235us/step - loss: 0.0596\n",
      "Epoch 93/100\n",
      "147569/147569 [==============================] - 35s 236us/step - loss: 0.0589\n",
      "Epoch 94/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0601\n",
      "Epoch 95/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0581\n",
      "Epoch 96/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0568\n",
      "Epoch 97/100\n",
      "147569/147569 [==============================] - 35s 236us/step - loss: 0.0584\n",
      "Epoch 98/100\n",
      "147569/147569 [==============================] - 34s 233us/step - loss: 0.0594\n",
      "Epoch 99/100\n",
      "147569/147569 [==============================] - 34s 232us/step - loss: 0.0586\n",
      "Epoch 100/100\n",
      "147569/147569 [==============================] - 35s 236us/step - loss: 0.0583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f08a3d7a90>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 512\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"potter_lstm_weights_0568.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"potter_lstm_weights_0568.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"s stupid, fat rat yellow.\" \n",
      "\n",
      "he waved hi\"\n",
      "s stupid, fat rat yellow.\" \n",
      "\n",
      "he waved his wifd, forg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mcama\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otte robblows and petunia field what he mied on its mind. the carent see than all day. it was hermione near thas he wooded mountain troll. and mone. o'd all the hat if i cold him to his face.\" \n",
      "\n",
      "\"look done this me somethin' all sitten yer muties.\" \n",
      "\n",
      "he said, \"ei'll can to expections chisers. ust past whise-here. it was only anyone found off his mother, they're not all in be a sholl tim\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"s stupid, fat rat yellow.\" \n",
      "\n",
      "he waved hi\"\n",
      "s stupid, fat rat yellow.\" \n",
      "\n",
      "he waved his wifd, because they weren't starting and there of from the mirror of erised quietly, bown here his lapped in a wazm. \"he are yound mation in the school, \"i'm said! ron and hermione. \n",
      "\n",
      "harry put his lang stopper meseley twiffing out with his really. he took a low ron dripped it fride mirs charlew hermione waited hather had even senkoned out of earing untious footstoppisdowed pleated on the ribbsar\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"s stupid, fat rat yellow.\" \n",
      "\n",
      "he waved hi\"\n",
      "s stupid, fat rat yellow.\" \n",
      "\n",
      "he waved his wird behind the first years feele as he could. \n",
      "\n",
      "\"you two we've got to come, it,\" he said forward. \"than is -- all yelled your father arrivy yeh on, i was doing out what he's spind, too. it was a sistack start chiven's copparte cloak, he knew that gient stuck in his eyes on the door. \n",
      "\n",
      "\"well vernon, his if it was doing,\" he said suddenly and schoollegs and school riding stepped on the dangest as\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.2, 0.5, 1.0]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
