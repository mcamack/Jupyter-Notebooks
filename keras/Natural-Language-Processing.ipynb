{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook summarizes Natural Language Processing (NLP) as presented in the following resources:\n",
    "* [Stanford CS224n: NLP w/Deep Learning](http://web.stanford.edu/class/cs224n/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) has the goal of making computers \"understand\" human languages in order to perform some useful tasks.\n",
    "\n",
    "Human language is a symbolic/categorical signaling system. Large vocabulary creates a machine learning problem with extreme sparsity in word encodings.\n",
    "\n",
    "NLP is difficult because human language is complex and ambiguous. Interpretation depends on learning and then using situtational, contexual, world, visual knowledge about the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis** - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bag-of-words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task of predicting what word comes next. More mathematically, this means that given a sequence of words, compute the probability distribution of the next word\n",
    "\n",
    "given $x^{(1)}, x^{(2)},..., x^{(t)}$ then the probability of the next word $x^{(t+1)}$ is:  \n",
    "    $P(x^{(t+1)} = w_j | x^{(t)},...,x^{(1)})$ where  \n",
    "    $w_j$ is a word in the vocabulary $V = {w_1, ..., w_{|V|}}$\n",
    "\n",
    "A Language Model can also **generate text** by choosing the next most likely word after training has been completed. However, a 3-gram will not lead to very meaningful sentences, a further looking back n-gram is needed - this would increase the size of the model exponentially though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### n-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**n-gram** is a chunk of *n* consecutive words\n",
    "* unigram: \"there\", \"it\", \"goes\"\n",
    "* bigram: \"there it\", \"it goes\"\n",
    "* trigram: \"there it goes\"\n",
    "\n",
    "Collect statistics about different n-gram frequency and use that to predict the next word. Use some large \"corpus\" of text in order to produce the n-gram probabilities. The probabilities will reflect the text trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "~~this part doesn't matter, because~~ only these words ___\n",
    "\n",
    "* discard the first words\n",
    "* condition on the last (n-1) = 3 words\n",
    "\n",
    "$P(w_j|only~these~words) = \\frac{count(only~these~words~w_j)}{count(only~these~words)}$\n",
    "\n",
    "* if \"only these words\" appear 1000 times, followed by \"matter\" in 400 cases, then: $P(matter|only~these~words) = 0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Increasing the $n$ makes sparsity problems **worse** and the model size huge\n",
    "* if $only~these~words~w_j$ never occurs in the data, then $w_j$ has probability of 0\n",
    "* if $only~these~words$ never occurs in the data, then we can't calculate prob's for any $w_j$\n",
    "* have to store count for all possible n-grams, model size is $O(exp(n))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
