{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "This notebook will summarize convolutional neural networks (CNN) as presented in the following fantastic resources:\n",
    "* [Stanford CS231n CNN Course](http://cs231n.github.io/convolutional-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "The CNN is similar to a regular feed-forward NN, except it assumes that inputs are images which allows us to make certain assumptions throughout the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular NNs have an input layer, 1 or more hidden layers, and an output layer. These layers are dense and fully-connected, which leads to many connections, many weights, and many calculations. They do well with smaller sized images (32 x 32 x 3 = 3072 parameters) ... but do not scale well to full sized images (200 x 200 x 3 = 120,000 parameters). Unlike a regular NN, CNNs have neurons in 3-dimensions for **width, height, and depth**. However, Neurons will only be connected to a small region of the previous layer; it will not be fully-connected which saves on computations. The final output layer will be fully-connected and brought down to a smaller vector of class scores arranged along the depth dimension. For example, a 1x1x10 vector for a 10 category classification problem.\n",
    "<img src=\"images/CNN1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "Every CNN layer transforms a 3D input volume into an output 3D volume with some differentiable function that may or may not have parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Layers\n",
    "The 3 most popular layers for CNNs are: **Convolutional, Pooling,** and **Fully-Connected**\n",
    "\n",
    "Example architecture: [INPUT - CONV - RELU - POOL - FC]\n",
    "* INPUT [32x32x3] holds raw pixel values\n",
    "* CONV layer multiplies its weights by a small region of the input resulting in [32x32x12] if we use 12 filters\n",
    "* RELU layer applies elementwise activation function like max(0,x) leaving the volume unchanged\n",
    "* POOL downsamples along spatial dimensions (height, width) resulting in [16x16x12]\n",
    "* FC computes class scores resulting in [1x1x10]\n",
    "\n",
    "CONV/FC layers have parameters and are trained with gradient descent to minimize overall error  \n",
    "RELU/POOL layers don't and implement a fixed function\n",
    "\n",
    "CONV/FC/POOL have additional **hyperparameters**; RELU does not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- Convolutional Layer ------------------------------------------------------------------------------------\n",
    "##### Overview - Convolutional Layer\n",
    "This is the heavy lifting, core computational block of CNNs. It consists of a set of \"learnable\" filters which are small spatially (height x width), but extend through the full depth of input volume. For example, a first layer ConvNet **filter** for an RGB image could be size [5x5x3] which is 5x5 pixels by 3 color channels. The convolution process:\n",
    "* slides the filter across and down (typewriter style) the input\n",
    "* computes a dot product between its entries and the input values it's \"covering\"\n",
    "* produces a 2-D **activation map** showing that filter's responses\n",
    "* repeats in parallel the same process for other filters in the CONV layer filter set\n",
    "* stack the activation maps along depth dimension to produce an output volume\n",
    "\n",
    "The **receptive field** is the filter size and this is how many neurons we connect to in the previous layer. Spatially (height x width) this will be smaller than the input volume, but it ALWAYS covers the full depth on the input volume. If our input image had 12 color bands, then each neuron in the CONV layer will have weights for a 5x5x12 region in the input volume.\n",
    "\n",
    "##### Hyperparameters - Convolutional Layer\n",
    "These 4 values control the size of the output volume:  \n",
    "1. **Number of Filters (K)**, or depth, of the output volume corresponds to number of filters (each looks for different features like edges, gradients, colors, etc.). 5 filters would mean 5 different neurons looking at the same input region and they would form a **depth column**.\n",
    "<img src=\"images/depth.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "2. **Stride (S)** is how we move the filter. S=1 means we slide from pixel to pixel, while S=2 means we are skipping every other pixel. Larger strides result in smaller output volumes spatially (HxW).\n",
    "3. **Zero-Padding (P)** adds zeros around the border to control the spatial size of the output volume. Used to preserve a spatial output equal to the input.\n",
    "4. **Spatial Extent (F)** is the height (same as width) of the \"window\"\n",
    "\n",
    "Output volume spatial size = (W - F + 2P)/S + 1  \n",
    "* W = input volume size\n",
    "* F = receptive field size of CONV layer neurons\n",
    "* S = stride\n",
    "* P = zero padding on the border\n",
    "\n",
    "Ensure input & output volume are equal by setting padding P = (F-1)/2 when the stride is 1\n",
    "\n",
    "##### Parameter Sharing - Convolutional Layer\n",
    "Parameter Sharing is used in CONV layers to control the number of parameters that must be kept in memory when doing computations. We assume that a feature found at one spatial position will be useful in another, so instead of each neuron within a **depth slice** (filter layer) keeping separate weights & bias, we set them all to be the same. Now, every neuron in a depth slice has the same weights so we only have unique weights for each depth slice. Use an example where input image size is [227x227x3] with F=11, S=4, P=0, and K=96.\n",
    "* Without parameter sharing: first CONV layer will have [55x55x96] = 290,400 neurons with unique weights. Each of these neurons is connected to a [11x11x3] area on the input (each has 1 bias unit as well). This leave 290,400x364 = 105,705,600 parameters\n",
    "* With parameter sharing: first CONV layer will have 96 unique neuron weights (one for each depth slice/filter). Each of these connects to an [11x11x3] input area so we have 96x11x11x3 = 34,944 (+96 biases)\n",
    "\n",
    "With all neurons in a depth slice using the same weights, then the forward pass of each depth slice within the CONV layer is computed as a **convolution**. Then, we refer to the sets of weights as a filter, or **kernel**, that is convolved with the input.\n",
    "\n",
    "Parameter sharing makes sense when the features detected are simple like line and gradients, but when the spatial location matters (like looking for eyes on a portrait) then parameter sharing can be relaxed and we call the layer a **locally-connected layer**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- Pooling Layer -------------------------------------------------------------------------------------------\n",
    "##### Overview - Pooling Layer\n",
    "To progressively reduce the spatial size and reduce the number of parameters in a ConvNet, **Pooling** layers are inserted between CONV layers. This reduces the amount of computation required and helps control overfitting. The POOL layer operates independently on each depth slice of the input and resizes it spatially using the MAX operation. \n",
    "\n",
    "##### Hyperparameters - Convolutional Layer\n",
    "These 2 values determine how much we are going to reduce the ConvNet:\n",
    "1. **Spatial Extent (F)** is the size of the window\n",
    "2. **Stride (S)** is how many pixels we move the window\n",
    "\n",
    "POOL layer accepts an input of [WxHxD] and produces an output of:\n",
    "* W' = (W-F)/S + 1\n",
    "* H' = (H-F)/S + 1\n",
    "* D' = D\n",
    "\n",
    "This does not introduce any parameters (removes them actually) and we usually do not Pad the POOL layer. In practice, there are only 2 types of MAX POOL layers used:\n",
    "1. F=2, S=2\n",
    "2. F=3, S=2 (overlapping pooling)\n",
    "\n",
    "<img src=\"images/POOL.png\" alt=\"Drawing\" style=\"width: 750px;\"/>\n",
    "The example above shows type 1, which slides a 2x2 windows across the input with stride 2 (not overlapping) and outputs the max value it sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
