{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Overview\n",
    "High-level neural network Python package using TensorFlow as a backend\n",
    "* CPU or GPU versions\n",
    "* CNTK and Theano can be used as the backend instead\n",
    "\n",
    "Quickly build Neural Networks in a declarative manner\n",
    "* i.e. tell Keras **what** kind of NN model/layers you want\n",
    "* Supports Convolutional (CNN) and Recurrent (RNN) neural networks (or a combo)\n",
    "* Using TensorFlow itself would give more control of **how** to build a fine-tuned NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "Two types of implementations for Models:\n",
    "* The **Sequential** model is the standard way of building a NN by linearly stacking layers\n",
    "* The **Model Class** can be used to create a more complex NN using the Keras functional API\n",
    "\n",
    "## Layers\n",
    "NNs have an input layer, 1 or more hidden layers, and an output layer. Each layer also has an activation function applied to each node which is how learning takes place. Each node uses this function to compute a weighted sum of how much the input contributes to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "### Overview\n",
    "The Activation Function (AF) is a weighted, mathematical function applied to the inputs, which results in a final output \"score\". It could output infinity and completely outweigh any other node if they have smaller values, that's bad. A more practical approach is to apply the same AF to each node and normalize the range to something like -1 to 1. Now they're all on the same playing field in terms of contribution to the final output layer. The actual function can be just about anything, from simple to complex:\n",
    "* **Step Function**: simple and blunt, yes or no, output is 0 or 1. This is easy to understand, but not the best for a NN. This AF either says, \"Yes, my input contributes info to the output, or no it does not.\" There is no maybe, a little bit, or sort of. This is not the best way to gather info from multiple sources because each one likely contributes something to the final result and we want to compile that info as we move through a deep neural network with many layers.\n",
    "* **Linear Function**: a straight line, input is proportional to output. This is better, but still bad for three reasons. (1) the output is not bound and could \"blow up\" to a huge number, or very tiny number, which isn't useful. (2) if every layer has a linear AF, then the whole system is linear and there's no point in having multiple layers, it's really just like one linear function once combined. (3) when we get to back propagation for gradient descent and minimizing the error, the gradient (derivative of the line) is constant and doesn't depend on the input\n",
    "* **Sigmoid Function**: 'S' shaped, this is better, it's not all or nothing like Step and it is non-linear so multiple layers are more meaningful. Small input changes (x) have a larger affect on the output (y), that's good! It causes this node to eventually figure out whether how to classify the output by drifting towards y=0 or y=1. However, towards the ends of the function (small or large input values) the output has very little change and is nearly flatlined. This means the node is set in its ways and nothing you say can change its mind! That may be good, or bad, but this AF is widely used.\n",
    "* **Tanh Function**: this is a scaled version of the Sigmoid function, so it has similar characteristics. The difference is that the gradients are even steeper\n",
    "* **ReLu Function**: this name is used all the time, what is it? The Rectified Linear Unit is linear if x is greater than 0, otherwise it outputs zero. Overall, it *IS* non-linear and it can speed up deep NN processing by making it less computationally dense. Meaning, if it outputs a zero, then there is less number crunching at the next layer because it's input is zero. However, with a flat-line zero gradient for x < 0, the node will stop responding to variations in input. This causes a dying ReLu neuron which makes this portion of the NN go passive. To remedy this, the flat line for x < 0 can be made into a small slope so it will gradually recover during training (Leaky ReLu) instead of causing that whole portion of the NN to become unresponsive to change.\n",
    "![title](AF.png)\n",
    "### Keras Activation Function usage\n",
    "  Add an AF to each forward layer in 1 of 2 ways:\n",
    " 1. model.add(Dense(64, activation='tanh'))\n",
    " 2. model.add(Dense(64)); model.add(Activation('tanh')\n",
    "\n",
    "  [Available activation functions](https://keras.io/activations/#available-activations):  \n",
    " 1. **elu**\n",
    " 2. **selu**\n",
    " 3. **softplus**\n",
    " 4. **softsign**\n",
    " 5. **relu**\n",
    " 6. **tanh**\n",
    " 7. **sigmoid**\n",
    " 8. **hardsigmoid**\n",
    " 9. **linear**\n",
    " 10. **softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Sequential model made up of 2 layers\n",
    "* The 1st layer must have a defined input shape: input_shape=(X,)   or   input_dim=X\n",
    " *\n",
    "* The 2nd layer (output layer) \n",
    " * following layers can infer what shape they should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "model.add(Dense(units=64, input_dim=100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "one_hot_labels = to_categorical(labels, num_classes=10) # Convert labels to categorical one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.5803 - acc: 0.3870        \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.8605 - acc: 0.5000     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7693 - acc: 0.4990     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7425 - acc: 0.4980     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7276 - acc: 0.5130     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7208 - acc: 0.5110     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7170 - acc: 0.5020     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7112 - acc: 0.5210     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7057 - acc: 0.5220     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7059 - acc: 0.5330     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x237e54a3518>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
