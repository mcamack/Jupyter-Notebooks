{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Overview\n",
    "High-level neural network Python package using TensorFlow as a backend\n",
    "* CPU or GPU versions\n",
    "* CNTK and Theano can be used as the backend instead\n",
    "\n",
    "Quickly build Neural Networks in a declarative manner\n",
    "* i.e. tell Keras **what** kind of NN model/layers you want\n",
    "* Supports Convolutional (CNN) and Recurrent (RNN) neural networks (or a combo)\n",
    "* Using TensorFlow itself would give more control of **how** to build a fine-tuned and customized NN\n",
    "\n",
    "Great References:  \n",
    "* [Stanford CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/neural-networks-1/)\n",
    "* [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)\n",
    "* [Neural Network Zoo](http://www.asimovinstitute.org/neural-network-zoo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "Two types of implementations for Models:\n",
    "* The **Sequential** model is the standard way of building a NN by linearly stacking layers\n",
    "* The **Model Class** can be used to create a more complex NN using the Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "### Overview\n",
    "Neural Networks have an input layer, 1 or more hidden layers, and an output layer. Each layer also has an activation function applied to the output of each node which determines how much its inputs contribute to the output. Deep Neural Networks would have many hidden layers and/or many nodes in each one.\n",
    "\n",
    "![title](NN.png)  \n",
    "There are differing types of layers which can be added, each one performing a different task for the overall NN architecture. Varying the sizes and types of layers will create specialized NNs. This colorful picture shows what kind of complexity can be created!  \n",
    "![title](ZOO.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Core Layer Usage\n",
    "Add a forward layer and it's activation function with:\n",
    "* model.add(**Dense(units=64, input_dim=100)**)\n",
    "* model.add(**Activation('relu')**)\n",
    "\n",
    "[Available core layers:](https://keras.io/layers/core/)\n",
    "* **Dense**: a standard, fully-connected, dense NN layer\n",
    "* **Activation**: see below. This is applied to the output of a layer and is a property of other layers\n",
    "* **Dropout**: randomly sets a fraction of inputs to 0 to prevent overfitting and speed up training\n",
    "* **Flatten**: \n",
    "* **Reshape**\n",
    "* **Permute**\n",
    "* **RepeatVector**\n",
    "* **Lambda**\n",
    "* **ActivityRegularization**\n",
    "* **Masking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "model.add(Dense(units=64, input_dim=100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "### Overview\n",
    "The Activation Function (AF) is a weighted, mathematical function applied to the inputs, which results in a final output \"score\". It could output infinity and completely outweigh any other node if they have smaller values, that's bad. A more practical approach is to apply the same AF to each node and normalize the range to something like -1 to 1. Now they're all on the same playing field in terms of contribution to the final output layer. The actual function can be just about anything, from simple to complex:\n",
    "* **Step Function**: simple and blunt, yes or no, output is 0 or 1. This is easy to understand, but not the best for a NN. This AF either says, \"Yes, my input contributes info to the output, or no it does not.\" There is no maybe, a little bit, or sort of. This is not the best way to gather info from multiple sources because each one likely contributes something to the final result and we want to compile that info as we move through a deep neural network with many layers.\n",
    "* **Linear Function**: a straight line, input is proportional to output. This is better, but still bad for three reasons. (1) the output is not bound and could \"blow up\" to a huge number, or very tiny number, which isn't useful. (2) if every layer has a linear AF, then the whole system is linear and there's no point in having multiple layers, it's really just like one linear function once combined. (3) when we get to back propagation for gradient descent and minimizing the error, the gradient (derivative of the line) is constant and doesn't depend on the input\n",
    "* **Sigmoid Function**: 'S' shaped, this is better, it's not all or nothing like Step and it is non-linear so multiple layers are more meaningful. Small input changes (x) have a larger affect on the output (y), that's good! It causes this node to eventually figure out whether how to classify the output by drifting towards y=0 or y=1. However, towards the ends of the function (small or large input values) the output has very little change and is nearly flatlined. This means the node is set in its ways and nothing you say can change its mind! That may be good, or bad, but this AF is widely used.\n",
    "* **Tanh Function**: this is a scaled version of the Sigmoid function, so it has similar characteristics. The difference is that the gradients are even steeper\n",
    "* **ReLu Function**: this name is used all the time, what is it? The Rectified Linear Unit is linear if x is greater than 0, otherwise it outputs zero. Overall, it *IS* non-linear and it can speed up deep NN processing by making it less computationally dense. Meaning, if it outputs a zero, then there is less number crunching at the next layer because it's input is zero. However, with a flat-line zero gradient for x < 0, the node will stop responding to variations in input. This causes a dying ReLu neuron which makes this portion of the NN go passive. To remedy this, the flat line for x < 0 can be made into a small slope so it will gradually recover during training (Leaky ReLu) instead of causing that whole portion of the NN to become unresponsive to change.\n",
    "![title](AF.png)\n",
    "\n",
    "### Keras Activation Function Usage\n",
    "  Add an AF to each forward layer in 1 of 2 ways:\n",
    " 1. model.add(Dense(64, **activation='tanh'**))\n",
    " 2. model.add(Dense(64)); **model.add(Activation('tanh')**  \n",
    "\n",
    "[Available activation functions](https://keras.io/activations/#available-activations):  \n",
    " 1. **elu**\n",
    " 2. **selu**\n",
    " 3. **softplus**\n",
    " 4. **softsign**\n",
    " 5. **relu**\n",
    " 6. **tanh**\n",
    " 7. **sigmoid**\n",
    " 8. **hardsigmoid**\n",
    " 9. **linear**\n",
    " 10. **softmax**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Sequential model made up of 2 layers\n",
    "* The 1st layer must have a defined input shape: input_shape=(X,)   or   input_dim=X\n",
    " *\n",
    "* The 2nd layer (output layer) \n",
    " * following layers can infer what shape they should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "one_hot_labels = to_categorical(labels, num_classes=10) # Convert labels to categorical one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.2437 - acc: 0.4380     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.8099 - acc: 0.5040     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7592 - acc: 0.5120     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7419 - acc: 0.4940     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7301 - acc: 0.5090     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7246 - acc: 0.5100     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7175 - acc: 0.5160     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7144 - acc: 0.5080     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7104 - acc: 0.5050     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7070 - acc: 0.5280     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a7d99a2550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
